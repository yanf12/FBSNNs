{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from scipy.special import comb\n",
    "from scipy.stats import norm\n",
    "from common_tools import neural_networks\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def theoretical_vanilla_eu(S0=50, K=50, T=1, r=0.05, sigma=0.4, type_='call'):\n",
    "    if T == 0:\n",
    "        if type_ == \"call\":\n",
    "            return max(S0 - K, 0)\n",
    "        else:\n",
    "            return max(K - S0, 0)\n",
    "    d1 = ((np.log(S0 / K) + (r + 0.5 * sigma ** 2) * T)) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    if type_ == \"call\":\n",
    "        c = S0 * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "        return c\n",
    "    elif type_ == \"put\":\n",
    "        p = K * np.exp(-r * T) * norm.cdf(-d2) - S0 * norm.cdf(-d1)\n",
    "        return p\n",
    "\n",
    "class neural_net(nn.Module):\n",
    "    def __init__(self, pathbatch=100, n_dim=100 + 1, n_output=1):\n",
    "        super(neural_net, self).__init__()\n",
    "        self.width = 1024\n",
    "        self.pathbatch = pathbatch\n",
    "        self.fc_1 = nn.Linear(n_dim, self.width)\n",
    "        self.fc_2 = nn.Linear(self.width, self.width)\n",
    "        self.fc_3 = nn.Linear(self.width, self.width)\n",
    "        self.fc_4 = nn.Linear(self.width, self.width)\n",
    "        self.fc_5 = nn.Linear(self.width, self.width)\n",
    "        self.fc_6 = nn.Linear(self.width, self.width)\n",
    "        self.fc_7 = nn.Linear(self.width, self.width)\n",
    "        self.fc_8 = nn.Linear(self.width, self.width)\n",
    "        self.fc_9 = nn.Linear(self.width, self.width)\n",
    "        self.fc_10 = nn.Linear(self.width, self.width)\n",
    "        self.out = nn.Linear(self.width, n_output)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.activation = self.relu\n",
    "\n",
    "        with torch.no_grad():\n",
    "            nn.init.xavier_uniform_(self.fc_1.weight)\n",
    "            nn.init.xavier_uniform_(self.fc_2.weight)\n",
    "            nn.init.xavier_uniform_(self.fc_3.weight)\n",
    "            nn.init.xavier_uniform_(self.fc_4.weight)\n",
    "            nn.init.xavier_uniform_(self.fc_5.weight)\n",
    "            nn.init.xavier_uniform_(self.fc_6.weight)\n",
    "            nn.init.xavier_uniform_(self.fc_7.weight)\n",
    "            nn.init.xavier_uniform_(self.fc_8.weight)\n",
    "            nn.init.xavier_uniform_(self.fc_9.weight)\n",
    "            nn.init.xavier_uniform_(self.fc_10.weight)\n",
    "\n",
    "    def forward(self, state, train=False):\n",
    "        state = self.activation(self.fc_1(state))\n",
    "        state = self.activation(self.fc_2(state))\n",
    "        state = self.activation(self.fc_3(state))\n",
    "        state = self.activation(self.fc_4(state))\n",
    "        state = self.activation(self.fc_5(state))\n",
    "        state = self.activation(self.fc_6(state))\n",
    "        state = self.activation(self.fc_7(state))\n",
    "        state = self.activation(self.fc_8(state))\n",
    "        state = self.activation(self.fc_9(state))\n",
    "        state = self.activation(self.fc_10(state))\n",
    "        fn_u = self.out(state)\n",
    "        return fn_u\n",
    "\n",
    "class FBSNN(nn.Module):\n",
    "    def __init__(self, r, K, sigma, Xi, T, M, N, D, learning_rate, gbm_scheme, lambda_, out_of_sample_input, out_of_sample_exact):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.sigma = sigma\n",
    "        self.K = K\n",
    "        self.Xi = Xi\n",
    "        self.T = T\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.fn_u = neural_net(pathbatch=M, n_dim=D + 1, n_output=1).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.fn_u.parameters(), lr=learning_rate)\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=500, gamma=0.6)\n",
    "\n",
    "        self.lambda_ = lambda_\n",
    "        self.gbm_scheme = gbm_scheme\n",
    "        self.out_of_sample_input = out_of_sample_input\n",
    "        self.out_of_sample_exact = out_of_sample_exact\n",
    "    def theoretical_vanilla_eu(self,S0=50, K=50, T=1, r=0.05, sigma=0.4, type_='call'):\n",
    "\n",
    "        if T == 0:\n",
    "            if type_ == \"call\":\n",
    "                return max(S0 - K, 0)\n",
    "            else:\n",
    "                return max(K - S0, 0)\n",
    "        # 求BSM模型下的欧式期权的理论定价\n",
    "        d1 = ((np.log(S0 / K) + (r + 0.5 * sigma ** 2) * T)) / (sigma * np.sqrt(T))\n",
    "        d2 = d1 - sigma * np.sqrt(T)\n",
    "        if type_ == \"call\":\n",
    "            c = S0 * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "            return c\n",
    "        elif type_ == \"put\":\n",
    "            p = K * np.exp(-r * T) * norm.cdf(-d2) - S0 * norm.cdf(-d1)\n",
    "            return p\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def phi_torch(self, t, X, Y, DuDx,DuDt,D2uDx2 ):  # M x 1, M x D, M x 1, M x D\n",
    "\n",
    "        res = DuDx*self.r*X+DuDt + 0.5*D2uDx2*X**2*self.sigma**2\n",
    "        return  res # M x 1\n",
    "\n",
    "    def g_torch(self, X,K):  # M x D\n",
    "\n",
    "        row_max, _ = torch.max(X, dim=1)  # get maximum along each row\n",
    "\n",
    "        return torch.clamp(row_max - K, min=0).unsqueeze(1)  # M x 1\n",
    "    def mu_torch(self, r,t, X, Y):  # 1x1, M x 1, M x D, M x 1, M x D\n",
    "        return 0*torch.ones([self.M, self.D])  # M x D\n",
    "\n",
    "    def sigma_torch(self, t, X, Y):  # M x 1, M x D, M x 1\n",
    "        # print(\"sigma_torch\")\n",
    "        # print(X.shape)\n",
    "        return self.sigma * torch.diag_embed(X)  # M x D x D\n",
    "\n",
    "    def net_u_Du(self, t, X):  # M x 1, M x D\n",
    "\n",
    "        inputs = torch.cat([t, X], dim=1)\n",
    "\n",
    "        u = self.fn_u(inputs)\n",
    "\n",
    "\n",
    "\n",
    "        DuDx = torch.autograd.grad(torch.sum(u), X, retain_graph=True,create_graph=True)[0]\n",
    "\n",
    "        # print(DuDx.shape)\n",
    "\n",
    "        DuDt = torch.autograd.grad(torch.sum(u), t, retain_graph=True,create_graph=True)[0]\n",
    "\n",
    "        D2uDx2 = torch.autograd.grad(torch.sum(DuDx), X, retain_graph=True,create_graph=True)[0]\n",
    "\n",
    "        return u, DuDx,DuDt,D2uDx2 # M x 1, M x D, M x 1, M x D\n",
    "\n",
    "    def Dg_torch(self, X):  # M x D\n",
    "        return torch.autograd.grad(torch.sum(self.g_torch(X,self.K)), X, retain_graph=True)[0]  # M x D\n",
    "\n",
    "    def fetch_minibatch(self):\n",
    "        T = self.T\n",
    "        M = self.M\n",
    "        N = self.N\n",
    "        D = self.D\n",
    "\n",
    "        Dt = np.zeros((M, N + 1, 1))  # M x (N+1) x 1\n",
    "        DW = np.zeros((M, N + 1, D))  # M x (N+1) x D\n",
    "\n",
    "        dt = T / N\n",
    "        Dt[:, 1:, :] = dt\n",
    "        DW[:, 1:, :] = np.sqrt(dt) * np.random.normal(size=(M, N, D))\n",
    "\n",
    "        t = np.cumsum(Dt, axis=1)  # M x (N+1) x 1\n",
    "        W = np.cumsum(DW, axis=1)  # M x (N+1) x D\n",
    "\n",
    "        return torch.from_numpy(t).float(), torch.from_numpy(W).float()\n",
    "\n",
    "    def loss_function(self, t, W, Xi):  # M x (N+1) x 1, M x (N+1) x D, MxD\n",
    "        loss = torch.zeros(1)\n",
    "        X_buffer = []\n",
    "        Y_buffer = []\n",
    "\n",
    "        t0 = t[:, 0, :]  # M x 1\n",
    "        W0 = W[:, 0, :]  # M x D\n",
    "        # X0 = torch.tensor([np.linspace(0.5,1.5,self.M)]).transpose(-1,-2).float()  # M x D\n",
    "        # X0 = torch.cat([Xi] * self.M)  # M x D\n",
    "        X0 = Xi\n",
    "\n",
    "        X0.requires_grad = True\n",
    "\n",
    "        t0.requires_grad = True\n",
    "        Y0, DuDx0,DuDt0,D2uDx20 = self.net_u_Du(t0, X0)  # M x 1, M x D\n",
    "\n",
    "        X_buffer.append(X0)\n",
    "        Y_buffer.append(Y0)\n",
    "        total_weight = 0\n",
    "\n",
    "        for n in range(0, self.N):\n",
    "            t1 = t[:, n + 1, :]\n",
    "            W1 = W[:, n + 1, :]\n",
    "\n",
    "            #\n",
    "            # print(\"t1-t0\")\n",
    "            # print((t1 - t0).shape)\n",
    "            #\n",
    "            # print(\"mu_torch:\")\n",
    "            # print(self.mu_torch(t0, X0, Y0, Z0).shape)\n",
    "            # print(\"sigma_torch:\")\n",
    "            # print(self.sigma_torch(t0, X0, Y0).shape)\n",
    "            # print(\"W1 - W0:\")\n",
    "            # print((W1 - W0).unsqueeze(-1).shape)\n",
    "            # print(torch.matmul(self.sigma_torch(t0, X0, Y0), (W1 - W0).unsqueeze(-1)).squeeze(2).shape)\n",
    "\n",
    "            if self.gbm_scheme ==0:\n",
    "                X1 = X0 + self.r*X0*(t1-t0) + self.sigma * X0 * (W1 - W0) # Euler-M scheme\n",
    "            elif self.gbm_scheme ==1:\n",
    "                X1 = X0*torch.exp( (self.r-0.5*self.sigma**2)*(t1-t0) + self.sigma* (W1-W0))\n",
    "\n",
    "            # print(X1.shape)\n",
    "\n",
    "            t1.requires_grad = True\n",
    "            Y1, DuDx1,DuDt1,D2uDx21 = self.net_u_Du(t1, X1)  # M x 1, M x D\n",
    "\n",
    "            Y1_tilde = Y0 + self.r*Y0* (t1-t0) + DuDx0 * self.sigma*X0*(W1-W0)\n",
    "\n",
    "            loss = loss + torch.sum((Y1 - Y1_tilde) ** 2)\n",
    "            total_weight +=1\n",
    "\n",
    "            t0 = t1\n",
    "            W0 = W1\n",
    "            X0 = X1\n",
    "            Y0 = Y1\n",
    "            DuDx0 = DuDx1 # not sure if this is correct\n",
    "            DuDt0 = DuDt1\n",
    "            D2uDx20 = D2uDx21\n",
    "\n",
    "            X_buffer.append(X0)\n",
    "            Y_buffer.append(Y0)\n",
    "\n",
    "        loss = loss + self.lambda_*torch.sum((Y1 - self.g_torch(X1,self.K)) ** 2)\n",
    "        total_weight += self.lambda_\n",
    "        loss = loss/total_weight\n",
    "        #loss = loss + torch.sum((Z1 - self.Dg_torch(X1)) ** 2)\n",
    "\n",
    "        X = torch.stack(X_buffer, dim=1)  # M x N x D\n",
    "        Y = torch.stack(Y_buffer, dim=1)  # M x N x 1\n",
    "\n",
    "        return loss, X, Y, Y[0, 0, 0]\n",
    "\n",
    "    def train(self, N_Iter=10):\n",
    "        loss_list = []\n",
    "        error_list = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        t = np.linspace(0, 1, 10)\n",
    "        S = np.linspace(0, 2, 10)\n",
    "        test_sample_list = []\n",
    "\n",
    "        t_mesh, S_mesh = np.meshgrid(t, S)\n",
    "\n",
    "        mse_list = []\n",
    "        mae_list = []\n",
    "        for it in range(N_Iter):\n",
    "\n",
    "\n",
    "            t_batch, W_batch = self.fetch_minibatch()  # M x (N+1) x 1, M x (N+1) x D\n",
    "            loss, X_pred, Y_pred, Y0_pred = self.loss_function(t_batch, W_batch, self.Xi)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            loss_list.append(loss.detach().numpy()[0])\n",
    "\n",
    "            test_sample_list.append(self.fn_u(self.out_of_sample_input).detach().numpy()[0])\n",
    "\n",
    "\n",
    "            NN_price_surface = np.zeros([10, 10])\n",
    "            Exact_price_surface = np.zeros([10, 10])\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    NN_price_surface[i, j] = model.fn_u(\n",
    "                        torch.tensor([[t_mesh[i, j], S_mesh[i, j]]]).float()).detach().numpy()\n",
    "                    Exact_price_surface[i, j] = self.theoretical_vanilla_eu(S0=S_mesh[i, j], K=1, T=1 - t_mesh[i, j],\n",
    "                                                                            r=0.05, sigma=0.4, type_='call')\n",
    "\n",
    "            Error_measure = neural_networks.errormeasure(Exact_price_surface, NN_price_surface)\n",
    "            mse = Error_measure.calculate_mse()\n",
    "            mae = Error_measure.calculate_mae()\n",
    "            mape = Error_measure.calculate_mape()\n",
    "            mse_list.append(mse)\n",
    "            mae_list.append(mae)\n",
    "            # Print\n",
    "            if it % 202 == 0:\n",
    "                clear_output(wait=True)\n",
    "                elapsed = time.time() - start_time\n",
    "                print('It: %d, Time: %.2f, Loss: %.3e, Y0: %.3f' %\n",
    "                      (it, elapsed, loss, Y0_pred))\n",
    "                start_time = time.time()\n",
    "                plt.plot(np.log10(range(len(loss_list))),np.log10(loss_list))\n",
    "                plt.show()\n",
    "\n",
    "                plt.plot(test_sample_list)\n",
    "                plt.plot(np.ones(len(test_sample_list)) * test_sample_exact, label='test sample exact price')\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "                error_surface = np.abs(NN_price_surface - Exact_price_surface)\n",
    "                # error_list.append(np.max(error_surface))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                ax = plt.figure()\n",
    "                ax = plt.axes(projection='3d')\n",
    "                ax.plot_surface(t_mesh, S_mesh, NN_price_surface, rstride=1, cstride=1, cmap='viridis',\n",
    "                                edgecolor='none')\n",
    "                ax.set_title('surface: iter = %d' % it)\n",
    "                plt.show()\n",
    "\n",
    "                # ax = plt.figure()\n",
    "                # ax = plt.axes(projection='3d')\n",
    "                # ax.plot_surface(t_mesh, S_mesh, error_surface, rstride=1, cstride=1, cmap='viridis',\n",
    "                #                 edgecolor='none')\n",
    "                # ax.set_title('error surface: iter = %d' % it)\n",
    "                # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        self.loss_list = loss_list\n",
    "        self.test_sample_list = test_sample_list\n",
    "        self.mse_list = mse_list\n",
    "        self.mae_list = mae_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, Xi_star, t_star, W_star):\n",
    "        _, X_star, Y_star, _ = self.loss_function(t_star, W_star, Xi_star)\n",
    "\n",
    "        return X_star, Y_star\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     27\u001B[0m     Xi \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m1.0\u001B[39m, \u001B[38;5;241m0.5\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mint\u001B[39m(D \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m))[\u001B[38;5;28;01mNone\u001B[39;00m, :])\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     29\u001B[0m model \u001B[38;5;241m=\u001B[39m FBSNN(r, K, sigma, Xi, T, M, N, D, learning_rate, gbm_scheme\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, lambda_\u001B[38;5;241m=\u001B[39mlambda_, out_of_sample_input\u001B[38;5;241m=\u001B[39mout_of_sample_input, out_of_sample_exact\u001B[38;5;241m=\u001B[39mtest_sample_exact)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 31\u001B[0m t_batch, W_batch \u001B[38;5;241m=\u001B[39m \u001B[43mt_batch\u001B[49m\u001B[38;5;241m.\u001B[39mto(device), W_batch\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     33\u001B[0m Xi \u001B[38;5;241m=\u001B[39m Xi\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     37\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(N_Iter\u001B[38;5;241m=\u001B[39mepoch)\n",
      "\u001B[1;31mNameError\u001B[0m: name 't_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "if __name__ == '__main__':\n",
    "    M = 100  # number of trajectories (batch size)\n",
    "    N = 10  # number of time snapshots\n",
    "\n",
    "    learning_rate = 3.0 * 1e-3\n",
    "    epoch = 3000\n",
    "\n",
    "    r = 0.05\n",
    "    K = 1.0\n",
    "    T = 1.0\n",
    "    sigma = 0.4\n",
    "    D = 1  # number of dimensions\n",
    "    lambda_ = 500  # weight for BC\n",
    "    out_of_sample_test_t = 0.0\n",
    "    out_of_sample_test_S = 1.0\n",
    "\n",
    "    out_of_sample_input = torch.tensor([out_of_sample_test_t, out_of_sample_test_S]).float()\n",
    "    test_sample_exact = theoretical_vanilla_eu(out_of_sample_test_S, K, T - out_of_sample_test_t, r, sigma)\n",
    "    gbm_scheme = 1  # in theory 1 is more accurate. 0 is accurate for large N\n",
    "\n",
    "    if D == 1:\n",
    "        Xi = torch.tensor([np.linspace(0.0, 2.0, M)]).transpose(-1, -2).float()\n",
    "        #Xi = torch.ones([M,1])\n",
    "    else:\n",
    "        Xi = torch.from_numpy(np.array([1.0, 0.5] * int(D / 2))[None, :]).float()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def theoretical_vanilla_eu(S0=50, K=50, T=1, r=0.05, sigma=0.4, type_='call'):\n",
    "        '''\n",
    "        :param S0: 股票当前价格\n",
    "        :param K: 行权价格\n",
    "        :param T: 到期时间（年）\n",
    "        :param r: 无风险利率， 如 r = 0.05\n",
    "        :param sigma: 波动率， 如 sigma = 0.20\n",
    "        :param type_:  call or put\n",
    "        :return: 期权价格\n",
    "        '''\n",
    "        if T == 0:\n",
    "            if type_ == \"call\":\n",
    "                return max(S0 - K, 0)\n",
    "            else:\n",
    "                return max(K - S0, 0)\n",
    "        #求BSM模型下的欧式期权的理论定价\n",
    "        d1 = ((np.log(S0 / K) + (r + 0.5 * sigma ** 2) * T)) / (sigma * np.sqrt(T))\n",
    "        d2 = d1 - sigma * np.sqrt(T)\n",
    "        if type_ == \"call\":\n",
    "            c = S0 * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "            return c\n",
    "        elif type_ == \"put\":\n",
    "            p = K * np.exp(-r * T) * norm.cdf(-d2) - S0 * norm.cdf(-d1)\n",
    "\n",
    "            return p\n",
    "\n",
    "\n",
    "    def u_exact(t, X):  # (N+1) x 1, (N+1) x D\n",
    "        r = 0.05\n",
    "        sigma = 0.4\n",
    "        K = 1\n",
    "        T = 1\n",
    "        res = np.zeros([t.shape[0], X.shape[1]])\n",
    "        for i in range(t.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                res[i, j] = theoretical_vanilla_eu(S0=X[i, j], K=K, T=T - t[i, 0], r=r, sigma=sigma, type_='call')\n",
    "        return res\n",
    "\n",
    "\n",
    "    # Set the device to CUDA if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create the model and move it to the device\n",
    "    model = FBSNN(r, K, sigma, Xi, T, M, N, D, learning_rate, gbm_scheme=1, lambda_=lambda_, out_of_sample_input=out_of_sample_input, out_of_sample_exact=test_sample_exact).to(device)\n",
    "\n",
    "    # Move the data to the device\n",
    "    t_batch, W_batch = t_batch.to(device), W_batch.to(device)\n",
    "\n",
    "# Set the device for existing tensors\n",
    "Xi = Xi.to(device)\n",
    "\n",
    "\n",
    "    t_test = t_test.detach().numpy()\n",
    "    X_pred = X_pred.detach().numpy()\n",
    "    Y_pred = Y_pred.detach().numpy()\n",
    "    Y_test = np.reshape(u_exact(np.reshape(t_test[0:M, :, :], [-1, 1]), np.reshape(X_pred[0:M, :, :], [-1, D])),\n",
    "                        [M, -1, 1])\n",
    "    print(Y_test[0, 0, 0])\n",
    "\n",
    "    plt.figure(figsize=[9, 6])\n",
    "    plt.plot(test_sample_list[100:], label='NN output price')\n",
    "    plt.plot(np.ones(len(test_sample_list))[100:] * test_sample_exact, label='test sample exact price')\n",
    "    plt.title('Covergence of the price')\n",
    "    plt.xlabel(\"Epochs trained\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.log10(model.loss_list), label='loss')\n",
    "    plt.show()\n",
    "\n",
    "    samples = 30\n",
    "    plt.figure()\n",
    "    plt.plot(t_test[0:1, :, 0].T, Y_pred[0:1, :, 0].T, 'b', label=r'Learned $\\hat{V}(S_t,t)$')\n",
    "    plt.plot(t_test[0:1, :, 0].T, Y_test[0:1, :, 0].T, 'r--', label=r'Exact $V(S_t,t)$')\n",
    "    plt.plot(t_test[0:1, -1, 0], Y_test[0:1, -1, 0], 'ko', label=r'$V_T = V(S_T,T)$')\n",
    "\n",
    "    plt.plot(t_test[1:samples, :, 0].T, Y_pred[1:samples, :, 0].T, 'b')\n",
    "    plt.plot(t_test[1:samples, :, 0].T, Y_test[1:samples, :, 0].T, 'r--')\n",
    "    plt.plot(t_test[1:samples, -1, 0], Y_test[1:samples, -1, 0], 'ko')\n",
    "    plt.plot([0], Y_test[0, 0, 0], 'ks', label=r'$V_0 = V(S_0,0)$')\n",
    "    plt.xlabel(r'$t$')\n",
    "    plt.ylabel(r'$V_t$')\n",
    "    plt.title(r'Path of exact $V(S_t,t)$ and learned $\\hat{V}(S_t,t)$')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(\"figures/path plot 1d.png\", dpi=500)\n",
    "    plt.show()\n",
    "\n",
    "    # %%\n",
    "    t = np.linspace(0, 1, 10)\n",
    "    S = np.linspace(0, 2, 10)\n",
    "\n",
    "    t_mesh, S_mesh = np.meshgrid(t, S)\n",
    "\n",
    "    NN_price_surface = np.zeros([10, 10])\n",
    "    Exact_price_surface = np.zeros([10, 10])\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            NN_price_surface[i, j] = model.fn_u(torch.tensor([[t_mesh[i, j], S_mesh[i, j]]]).float()).detach().numpy()\n",
    "            Exact_price_surface[i, j] = theoretical_vanilla_eu(S0=S_mesh[i, j], K=1, T=1 - t_mesh[i, j], r=0.05,\n",
    "                                                               sigma=0.4, type_='call')\n",
    "    error_surface = NN_price_surface - Exact_price_surface\n",
    "    # Calculate percentage error, avoiding division by zero\n",
    "    percentage_error_surface = np.where(Exact_price_surface != 0, np.abs(error_surface) / Exact_price_surface * 100, 0)\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # First subplot for Neural Network price surface\n",
    "    ax1 = fig.add_subplot(2, 2, 1, projection='3d')\n",
    "    surf = ax1.plot_surface(t_mesh, S_mesh, NN_price_surface, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "    ax1.set_title('Neural Network Price Surface')\n",
    "    ax1.set_xlabel('Time ($t$)')\n",
    "    ax1.set_ylabel('Price ($S_t$)')\n",
    "    # fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)  # add color bar\n",
    "\n",
    "    # Second subplot for Exact price surface\n",
    "    ax2 = fig.add_subplot(2, 2, 2, projection='3d')\n",
    "    surf = ax2.plot_surface(t_mesh, S_mesh, Exact_price_surface, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "    ax2.set_title('Exact Price Surface')\n",
    "    ax2.set_xlabel('Time ($t$)')\n",
    "    ax2.set_ylabel('Price ($S_t$)')\n",
    "    # fig.colorbar(surf, ax=ax2, shrink=0.5, aspect=5)  # add color bar\n",
    "\n",
    "    # Third subplot for Error surface\n",
    "    ax3 = fig.add_subplot(2, 2, 3, projection='3d')\n",
    "    surf = ax3.plot_surface(t_mesh, S_mesh, np.abs(error_surface), rstride=1, cstride=1, cmap='viridis',\n",
    "                            edgecolor='none')\n",
    "    ax3.set_title('Absolute Error Surface')\n",
    "    ax3.set_xlabel('Time ($t$)')\n",
    "    ax3.set_ylabel('Price ($S_t$)')\n",
    "    # fig.colorbar(surf, ax=ax3, shrink=0.5, aspect=5)  # add color bar\n",
    "    # plt.savefig(\"figures/price surface 1d.png\", dpi=500)\n",
    "    plt.show()\n",
    "\n",
    "    # Fourth subplot for Percentage Error surface\n",
    "    errors = np.sqrt((Y_test - Y_pred) ** 2)\n",
    "    mean_errors = np.mean(errors, 0)\n",
    "    std_errors = np.std(errors, 0)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(t_test[0, :, 0], mean_errors, 'b', label='mean')\n",
    "    plt.plot(t_test[0, :, 0], mean_errors + 2 * std_errors, 'r--', label='mean + two standard deviations')\n",
    "    plt.xlabel(r'$t$')\n",
    "    plt.ylabel('relative error')\n",
    "    plt.title('100-dimensional Black-Scholes-Barenblatt')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #%%\n",
    "    error_surface = NN_price_surface - Exact_price_surface\n",
    "    ax = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(t_mesh, S_mesh, error_surface, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "    ax.set_title('Error surface')\n",
    "    plt.show()\n",
    "\n",
    "    mse_list = model.mse_list\n",
    "    mae_list = model.mae_list\n",
    "    plt.plot(mse_list, label='mse')\n",
    "    plt.plot(mae_list, label='mae')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #%%\n",
    "    from common_tools import neural_networks\n",
    "\n",
    "    Error_measure = neural_networks.errormeasure(Exact_price_surface, NN_price_surface)\n",
    "\n",
    "    mse = Error_measure.calculate_mse()\n",
    "    mae = Error_measure.calculate_mae()\n",
    "    mape = Error_measure.calculate_mape()\n",
    "\n",
    "    plt.plot(test_sample_list[50:])\n",
    "    plt.plot(np.ones(len(test_sample_list[50:])) * test_sample_exact, label='test sample exact price')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}